# _Python 爬蟲_

### 如何從亞洲大學資工系網站抓取教授個人資料

- [爬蟲教學](#爬蟲教學)
  - [網頁爬蟲介紹](#網頁爬蟲介紹)
  - [實例教學1](#實例教學1)
    - [第一步:下載](#第一步:下載)
      - [Python](#python)
      - [VS Code](#VSCode)
        - [VS Code安裝套件](#VSCode安裝套件)
      - [ChromeDriver](#ChromeDriver)
    - [第二步:安裝](#第二步:安裝)
      - [ pip](#pip)
      - [BeautifulSoup4](#BeautifulSoup4)
      - [Selenium](#Selenium)
    - [第三步:程式碼說明與配置](#第三步:程式碼說明與配置)
      - [引入模組](#引入模組)
      - [設定 ChromeDriver](#設定ChromeDriver)
      - [開啟目標網站與等待頁面載入](#開啟目標網站與等待頁面載入)
      - [模擬滾動頁面](#模擬滾動頁面)
      - [擷取 HTML 與關閉瀏覽器](#擷取HTML與關閉瀏覽器)
      - [儲存 HTML 以便除錯](#儲存HTML以便除錯)
      - [使用 BeautifulSoup 解析 HTML 並分區抓取內容](#使用BeautifulSoup解析HTML並分區抓取內容)
      - [儲存結果到文字檔](#儲存結果到文字檔)
  - [輸出結果](#輸出結果)
  - [可能遇到的問題與解決辦法](#可能遇到的問題與解決辦法)
    - [ChromeDriver 相容性問題](#ChromeDriver相容性問題)
    - [權限問題](#權限問題)
  - [實例教學2](#實例教學2)
    - [第一步:下載](#第一步:下載)
      - [Python](#python)
      - [VS Code](#VSCode)
        - [VS Code安裝套件](#VSCode安裝套件)
      - [ChromeDriver](#ChromeDriver)
    - [第二步:安裝](#第二步:安裝)
      - [ pip](#pip)
      - [BeautifulSoup4](#BeautifulSoup4)
      - [Selenium](#Selenium)
    - [第三步:程式碼說明與配置](#第三步:程式碼說明與配置)
      - [引入模組與參數處理](#引入模組與參數處理)
      - [啟動瀏覽器並載入頁面](#啟動瀏覽器並載入頁面)
      - [模擬捲動](#模擬捲動)
      - [解析 HTML 與建立 SQLite 資料表](#解析HTML與建立SQLite資料表)
      - [擷取各師資分類內容](#擷取各師資分類內容)
      - [儲存 HTML 以便除錯](#儲存HTML以便除錯)
      - [匯出 CSV](#匯出CSV)
  - [執行方式](#執行方式)
  - [輸出結果](#輸出結果)
  - [可能遇到的問題與解決辦法](#可能遇到的問題與解決辦法)
    - [ChromeDriver 相容性問題](#ChromeDriver相容性問題)
    - [權限問題](#權限問題)

## 網頁爬蟲介紹

網頁爬蟲是一種自動化工具，主要用於系統化抓取網站上的資訊。它從起始 URL 出發，下載並解析 HTML 文件，提取出網頁中的鏈接、文本、圖像等資訊，再透過循環抓取不斷擴大數據收集範圍。常用技術包括 HTTP 請求庫、HTML 解析庫、以及動態內容處理工具。此外，分散式爬蟲框架如 Scrapy 能夠有效處理大規模數據抓取。

在實際應用中，網頁爬蟲廣泛用於搜尋引擎資料索引、數據挖掘、新聞聚合及學術研究。然而，使用爬蟲時須遵守網站的 robots.txt 規範、避免過度訪問導致服務中斷，以及注意版權和隱私問題。隨著動態網頁和反爬技術的普及，爬蟲面臨更多技術和法律上的挑戰，但同時也在雲計算及分佈式系統的支持下不斷進步和發展。

## 實例教學1

### 第一步:下載
### Python

[Python下載](https://www.python.org/downloads/).

點擊上面Python下載進入網址，點擊Download Python 3.13.3

![python1.](image/python1..jpeg)

點擊
- Modify

![python2.](image/python2..jpeg)

勾選
- Documentation

- pip
     
- tcl/tk and IDLE
     
- Python test suite

點擊
- Next

![python3.](image/python3..jpeg)

勾選
- Create shortcuts for installed applications

- Download free-threaded binaries(experimental)

點擊
- Install

![python4.](image/python4..jpeg)

### VS Code

[VS Code下載](https://code.visualstudio.com/download)

點擊上面VS Code下載進入網址，點擊自己電腦的平台進行下載

![VSCode1.](image/VSCode1..jpeg)

點擊

- 我同意(A)

- 下一步(N)

![VScode2.](image/VScode2..jpeg)

勾選

- 建立桌面圖(D)

- 針對支援的檔案類型將Code註冊為編輯器

- 加入PATH中(重新啟動後生效)

![VSCode3.](image/VSCode3..jpeg)

點擊

- 安裝(I)

![VScode4.](image/VSCode4..jpeg)

建立一個新的資料夾 project

點擊資料夾project，並在上方欄位輸入cmd，然後按 Enter

![VSCode5.](image/VSCode5..jpeg)

進入cmd後，輸入指令 _code  ._，然後按 Enter   _(code跟 . 中間一定要空格)_

![VSCode6.](image/VSCode6..jpeg)

就可以打開以project資料夾作為起始目錄的VS Code專案

![VSCode7.](image/VSCode7..jpeg)

### VS Code安裝套件

點擊

- 左邊Extensions

![VSCode8.](image/VSCode8..jpeg)

點擊搜尋輸入，並下載

- Python

- Pylance

- Python Degubber

![VSCode9.](image/VSCode9..jpeg)

### ChromeDriver

目前安裝的Chrome版本查詢

在Chrome的網址列輸入 _chrome://version_，然後按 Enter

![ChromeDriver](image/ChromeDriver.jpeg)

[ChromeDriver下載](https://googlechromelabs.github.io/chrome-for-testing/#stable)

依據目前安裝的Chrome版本，點擊上面ChromeDriver下載進入網址，下載相對應版本的ChromeDriver

![ChromeDriver1.](image/ChromeDriver1..jpeg)

下載後解壓縮，會得到一個 _chromedriver.exe_ 檔案

將 _chromedriver.exe_ 放到一個方便管理的資料夾中

### 第二步:安裝
### pip

在命令提示字元中執行以下指令（請確保是在系統的命令提示字元中，而非 Python 互動模式）

這個指令安裝 pip

```bash
C:\Users\USER\AppData\Local\Programs\Python\Python313\python.exe -m ensurepip
```

驗證安裝

完成上述步驟之後，在命令提示字元執行：

```bash
C:\Users\USER\AppData\Local\Programs\Python\Python313\python.exe -m pip --version
```

如果成功顯示 pip 版本號，代表 pip 安裝成功

![pip](image/pip.jpeg)

### BeautifulSoup4

透過pip安裝 BeautifulSoup4

```bash
pip install beautifulsoup4
```

![BeautifulSoup4](image/BeautifulSoup4.jpeg)

### Selenium

透過pip安裝Selenium模組

```bash
C:\Users\USER\AppData\Local\Programs\Python\Python313\python.exe -m pip install selenium
```

![Selenium](image/Selenium.jpeg)

### 第三部:程式碼說明與配置
### 引入模組

```python
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
```

- Selenium相關模組：用於啟動瀏覽器、等待元素、執行JavaScript滾動等動作

- BeautifulSoup：用於解析從網頁中取回的HTML資料

- time模組：用於控制等待時機，確保網頁資料載入完成

### 設定 ChromeDriver

```python
chrome_driver_path = r"C:\Users\USER\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
service = Service(chrome_driver_path)
driver = webdriver.Chrome(service=service)
```

- 將下載後的 ChromeDriver 路徑填入變數中

- 建立 Service 物件並作為參數啟動 ChromeDriver

### 開啟目標網站與等待頁面載入

```python
url = "https://csie.asia.edu.tw/zh_tw/associate_professors_2"
driver.get(url)

WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
```

- driver.get(url) 指定要開啟的網頁 URL

- 使用 WebDriverWait 等待頁面主體 <body> 出現，保證頁面已開始載入

### 模擬滾動頁面

目標網頁可能有部分內容使用JavaScript動態載入，必須模擬頁面滾動

```python
last_height = driver.execute_script("return document.body.scrollHeight")
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1.5)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

time.sleep(2)
```

- 利用JavaScript取得初始頁面高度，再不斷模擬滾動到頁面底部

- 每次滾動後，等待 1.5 秒，確保新內容載入

- 若新滾動後高度不再增加，代表所有動態內容已經載入，程式跳出迴圈

### 擷取HTML與關閉瀏覽器

```python
html = driver.page_source
driver.quit()
```

- 取得完整的頁面 HTML 原始碼，並關閉瀏覽器以節省資源

### 儲存HTML以便除錯

```python
with open("page_debug.html", "w", encoding="utf-8") as f:
    f.write(html)
```

- 將完整頁面 HTML 儲存到 page_debug.html，方便檢查網頁結構或除錯

### 使用BeautifulSoup解析HTML並分區抓取內容

```python
soup = BeautifulSoup(html, "html.parser")
```

- 解析 HTML 資料，方便之後依據標題定位各個分區內容

```python
categories = ["系主任", "教授", "榮譽教授", "兼任教授"]

output_text = ""

for cat in categories:
    output_text += f"\n{'='*10} {cat} {'='*10}\n"
    
    header = soup.find(lambda tag: tag.name in ["h2", "h3", "h4"] and cat in tag.get_text())
    if header:
        section_contents = []
        
        sibling = header.find_next_sibling()
        while sibling:
            
            if sibling.name in ["h2", "h3", "h4"] and any(c in sibling.get_text() for c in categories):
                break
            
            text_piece = sibling.get_text(separator="\n", strip=True)
            if text_piece:
                section_contents.append(text_piece)
            sibling = sibling.find_next_sibling()
        if section_contents:
            section_text = "\n".join(section_contents)
            output_text += section_text + "\n"
        else:
            output_text += "此區域找不到任何內容或需要調整解析方式。\n"
    else:
        output_text += "找不到該分類的標題，請檢查網頁結構。\n"
```

- 程式中定義了四個分類 categories = ["系主任", "教授", "榮譽教授", "兼任教授"]，依次尋找這些分類在網頁中對應的標題（使用 h2、h3 或 h4 標籤），然後抓取緊接著標題後的兄弟元素內容直到遇到另一個分類的標題為止。取得的資料最終累積至 output_text 字串中

### 儲存結果到文字檔

```python
with open("faculty_full_content.txt", "w", encoding="utf-8") as f:
    f.write(output_text)

print("已成功抓取各分類所有內容並儲存到 faculty_full_content.txt")
```

- 將分區整理後的文字內容儲存到 faculty_full_content.txt，並在終端機顯示完成訊息

## 輸出結果

[輸出結果](https://github.com/wenhsin01/Python-/blob/main/faculty_full_content.txt)

點擊輸出結果，即可查看程式輸出結果

## 可能遇到的問題與解決辦法

### ChromeDriver 相容性問題

- 確保你下載的ChromeDriver版本與Chrome瀏覽器版本一致。如果出現錯誤訊息，請前往ChromeDriver官網重新下載匹配版本

### 權限問題

- 在 Windows 平台上，注意路徑字串中反斜線被當作特殊字元處理而造成路徑解析錯誤
  
  解決方法

  1.使用原始字串:在字串前加上 r
  
  ```python
  chrome_driver_path = r"C:\Users\USER\Downloads\chromedriver-win64\chromedriver-win64.exe"
  ```

  2.使用雙反斜線:每個反斜線使用兩個

  ```python
  chrome_driver_path = "C:\\Users\\USER\\Downloads\\chromedriver-win64\\chromedriver-win64.exe"
  ```
  
## 實例教學2

### 第一步:下載
### Python

[Python下載](https://www.python.org/downloads/).

點擊上面Python下載進入網址，點擊Download Python 3.13.3

![python1.](image/python1..jpeg)

點擊
- Modify

![python2.](image/python2..jpeg)

勾選
- Documentation

- pip
     
- tcl/tk and IDLE
     
- Python test suite

點擊
- Next

![python3.](image/python3..jpeg)

勾選
- Create shortcuts for installed applications

- Download free-threaded binaries(experimental)

點擊
- Install

![python4.](image/python4..jpeg)

### VS Code

[VS Code下載](https://code.visualstudio.com/download)

點擊上面VS Code下載進入網址，點擊自己電腦的平台進行下載

![VSCode1.](image/VSCode1..jpeg)

點擊

- 我同意(A)

- 下一步(N)

![VScode2.](image/VScode2..jpeg)

勾選

- 建立桌面圖(D)

- 針對支援的檔案類型將Code註冊為編輯器

- 加入PATH中(重新啟動後生效)

![VSCode3.](image/VSCode3..jpeg)

點擊

- 安裝(I)

![VScode4.](image/VSCode4..jpeg)

建立一個新的資料夾 project

點擊資料夾project，並在上方欄位輸入cmd，然後按 Enter

![VSCode5.](image/VSCode5..jpeg)

進入cmd後，輸入指令 _code  ._，然後按 Enter   _(code跟 . 中間一定要空格)_

![VSCode6.](image/VSCode6..jpeg)

就可以打開以project資料夾作為起始目錄的VS Code專案

![VSCode7.](image/VSCode7..jpeg)

### VS Code安裝套件

點擊

- 左邊Extensions

![VSCode8.](image/VSCode8..jpeg)

點擊搜尋輸入，並下載

- Python

- Pylance

- Python Degubber

![VSCode9.](image/VSCode9..jpeg)

### ChromeDriver

目前安裝的Chrome版本查詢

在Chrome的網址列輸入 _chrome://version_，然後按 Enter

![ChromeDriver](image/ChromeDriver.jpeg)

[ChromeDriver下載](https://googlechromelabs.github.io/chrome-for-testing/#stable)

依據目前安裝的Chrome版本，點擊上面ChromeDriver下載進入網址，下載相對應版本的ChromeDriver

![ChromeDriver1.](image/ChromeDriver1..jpeg)

下載後解壓縮，會得到一個 _chromedriver.exe_ 檔案

將 _chromedriver.exe_ 放到一個方便管理的資料夾中

### 第二步:安裝
### pip

在命令提示字元中執行以下指令（請確保是在系統的命令提示字元中，而非 Python 互動模式）

這個指令安裝 pip

```bash
C:\Users\USER\AppData\Local\Programs\Python\Python313\python.exe -m ensurepip
```

驗證安裝

完成上述步驟之後，在命令提示字元執行：

```bash
C:\Users\USER\AppData\Local\Programs\Python\Python313\python.exe -m pip --version
```

如果成功顯示 pip 版本號，代表 pip 安裝成功

![pip](image/pip.jpeg)

### BeautifulSoup4

透過pip安裝 BeautifulSoup4

```bash
pip install beautifulsoup4
```

![BeautifulSoup4](image/BeautifulSoup4.jpeg)

### Selenium

透過pip安裝Selenium模組

```bash
C:\Users\USER\AppData\Local\Programs\Python\Python313\python.exe -m pip install selenium
```

![Selenium](image/Selenium.jpeg)

### 第三部:程式碼說明與配置
### 引入模組與參數處理

```python
import os
import argparse
import sqlite3
import csv
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
```

- argparse：用來定義命令列參數（如 ChromeDriver 路徑、SQLite 檔案與 CSV 檔案儲存路徑）

- selenium 與相關模組：啟動 Chrome 瀏覽器、等待元素出現、執行 JavaScript

- BeautifulSoup：解析 HTML

- sqlite3、csv：資料庫與 CSV 檔案操作

```python
def parse_args():
    parser = argparse.ArgumentParser(description="爬取師資介紹並儲存至 SQLite 與 CSV")
    parser.add_argument("--driver-path", default=..., help="ChromeDriver 可執行檔路徑")
    parser.add_argument("--db-path",    default=..., help="SQLite 資料庫檔案路徑")
    parser.add_argument("--csv-path",   default=..., help="匯出 CSV 檔案路徑")
    return parser.parse_args()
```

- --driver-path：指定 ChromeDriver 的執行檔位置

- --db-path、--csv-path：預設在程式同目錄下建立 faculty.db、faculty_export.csv

### 啟動瀏覽器並載入頁面

```python
service = Service(args.driver_path)
driver = webdriver.Chrome(service=service)
driver.get(target_url)

WebDriverWait(driver, 15).until(
    EC.presence_of_element_located((By.TAG_NAME, "body"))
)
```

- 用 Service 建立驅動後，再啟動 webdriver.Chrome

- driver.get() 載入指定 URL

- WebDriverWait 搭配 expected_conditions 等待 <body> 標籤出現，確保頁面已載入完成

### 模擬捲動

```python
last_height = driver.execute_script("return document.body.scrollHeight")
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1.5)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height
time.sleep(2)
```

- 取得目前網頁高度 scrollHeight

- 持續捲動到最下方，等待 1.5 秒，再重新量測高度

- 若高度不再增加（表示已到底），就跳出迴圈

- 最後再多等 2 秒，確保所有動態內容都已載入

### 解析 HTML 與建立 sqlite 資料表

```python
html = driver.page_source
driver.quit()
soup = BeautifulSoup(html, "html.parser")

conn = sqlite3.connect(args.db_path)
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS faculty (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        category TEXT NOT NULL,
        content TEXT NOT NULL
    )
''')
cursor.execute('DELETE FROM faculty')
```

- driver.page_source 取得完整 HTML，之後關閉瀏覽器

- BeautifulSoup 解析 HTML，方便以標籤搜尋內容

- 建立或連線到 sqlite 資料庫，若 faculty 表格不存在則建立；並先清空舊資料

### 擷取各師資分類內容

```python
categories = ["系主任", "教授", "榮譽教授", "兼任教授"]
for cat in categories:
    header = soup.find(lambda tag: tag.name in ["h2","h3","h4"] and cat in tag.get_text())
    ...
    sib = header.find_next_sibling()
    while sib:
        if sib.name in ["h2","h3","h4"] and any(c in sib.get_text() for c in categories):
            break
        text = sib.get_text(separator="\n", strip=True)
        if text:
            contents.append(text)
        sib = sib.find_next_sibling()
    if contents:
        full_text = "\n".join(contents)
        cursor.execute(
            'INSERT INTO faculty (category, content) VALUES (?, ?)',
            (cat, full_text)
        )
```

- categories 列表定義要擷取的四種分類

- 先用 soup.find() 找到對應分類的標題標籤（h2/h3/h4）

- 再從該標題的下一個兄弟節點開始，逐一蒐集文字，直到遇到下一個分類標題為止

- 將蒐集到的多行文字以換行符號串接成一個字串，寫入 sqlite

### 匯出 CSV

```python
cursor.execute('SELECT * FROM faculty')
rows = cursor.fetchall()
cols = [d[0] for d in cursor.description]
with open(args.csv_path, 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(cols)
    writer.writerows(rows)
print(f"資料已儲存至 SQLite ({args.db_path})，並匯出 {len(rows)} 筆到 CSV 檔：{args.csv_path}")
conn.close()
```

- 從 faculty 表中撈出所有欄位與紀錄

- 先把欄位名稱寫入 CSV 的標頭列，接著把每一筆資料寫入

- 最後印出成功訊息，並關閉資料庫連線

## 執行方式

打開 Windows 命令提示字元 (CMD)

切換到專案資料夾

```bat
cd C:\Users\USER\Desktop\project
```

一次執行整行指令

```bat
python scrape_faculty.py
```

![BeautifulSoup4](image/BeautifulSoup4.jpeg)

## 輸出結果

[輸出結果](https://github.com/wenhsin01/Python-/blob/main/faculty_export%20(1).csv)

點擊輸出結果，即可查看程式輸出結果

## 可能遇到的問題與解決辦法

- 不要在 >>> 底下貼執行指令，而要在真正的 CMD 提示符下執行

- CMD 預設都會有，不用另外下載 PowerShell

- 若要一鍵執行，最簡單就是配一支 .bat，而不是改 Python 程式碼本身
